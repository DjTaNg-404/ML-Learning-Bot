from typing import List, Optional, Dict, Any
from langchain.schema import Document
import os
import sys

# ä¿®æ”¹å¯¼å…¥æ–¹å¼ï¼Œé¿å…ç›¸å¯¹å¯¼å…¥é—®é¢˜
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from vector_store import VectorStore
except ImportError:
    from .vector_store import VectorStore

try:
    from document_loader import DocumentLoader
except ImportError:
    from .document_loader import DocumentLoader

try:
    from rerank import rerank_base
except ImportError:
    from .rerank import rerank_base


class RetrievalSystem:
    def __init__(self, 
                 embedding_type: str = "siliconflow",  # é»˜è®¤ä½¿ç”¨siliconflow
                 model_name: str = "BAAI/bge-large-zh-v1.5",  # ä½¿ç”¨SiliconFlowæ¨èæ¨¡å‹
                 persist_path: Optional[str] = None,
                 chunk_size: int = 500,  # å‡å°‘chunk sizeä»¥é¿å…API 413é”™è¯¯
                 chunk_overlap: int = 100,  # ç›¸åº”å‡å°‘overlap
                 **embedding_kwargs):  # æ·»åŠ è¿™ä¸ªå‚æ•°
        """
        åˆå§‹åŒ–æ£€ç´¢ç³»ç»Ÿ
        
        Args:
            embedding_type: åµŒå…¥æ¨¡å‹ç±»å‹ï¼Œåªæ”¯æŒ "siliconflow"
            model_name: æ¨¡å‹åç§°ï¼Œæ¨èä½¿ç”¨ "BAAI/bge-large-zh-v1.5"
            persist_path: æŒä¹…åŒ–è·¯å¾„
            chunk_size: æ–‡æ¡£åˆ†å‰²å¤§å°ï¼Œé»˜è®¤500ä»¥é¿å…APIè¯·æ±‚è¿‡å¤§
            chunk_overlap: æ–‡æ¡£åˆ†å‰²é‡å ï¼Œé»˜è®¤100
            **embedding_kwargs: ä¼ é€’ç»™åµŒå…¥æœåŠ¡çš„å…¶ä»–å‚æ•°ï¼ˆå¦‚api_keyç­‰ï¼‰
        """
        self.vector_store = VectorStore(
            embedding_type=embedding_type,
            model_name=model_name,
            persist_path=persist_path,
            **embedding_kwargs  # ä¼ é€’é¢å¤–å‚æ•°
        )
        
        self.document_loader = DocumentLoader(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        self.is_initialized = False
    
    def build_index_from_files(self, file_paths: List[str]) -> bool:
        """
        ä»æ–‡ä»¶æ„å»ºç´¢å¼•
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            æ„å»ºæ˜¯å¦æˆåŠŸ
        """
        try:
            print("å¼€å§‹æ„å»ºæ£€ç´¢ç´¢å¼•...")
            
            # åŠ è½½æ–‡æ¡£
            all_documents = self.document_loader.process_multiple_files(file_paths)
            
            if not all_documents:
                print("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ–‡æ¡£")
                return False
            
            print(f"æˆåŠŸåŠ è½½ {len(all_documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
            
            # åˆ›å»ºå‘é‡å­˜å‚¨
            self.vector_store.create_from_documents(all_documents)
            self.is_initialized = True
            
            # å¦‚æœæŒ‡å®šäº†æŒä¹…åŒ–è·¯å¾„ï¼Œè‡ªåŠ¨ä¿å­˜
            if self.vector_store.persist_path:
                self.vector_store.save()
                print(f"ç´¢å¼•å·²ä¿å­˜åˆ°: {self.vector_store.persist_path}")
            
            print("æ£€ç´¢ç´¢å¼•æ„å»ºå®Œæˆ")
            return True
            
        except Exception as e:
            print(f"æ„å»ºç´¢å¼•æ—¶å‡ºé”™: {e}")
            return False
    
    def load_index(self, path: Optional[str] = None) -> bool:
        """
        åŠ è½½å·²æœ‰çš„ç´¢å¼•
        
        Args:
            path: ç´¢å¼•è·¯å¾„
            
        Returns:
            åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            self.vector_store.load(path)
            self.is_initialized = True
            print("ç´¢å¼•åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            print(f"åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
            return False
    
    def add_documents_from_files(self, file_paths: List[str]) -> bool:
        """
        å‘ç°æœ‰ç´¢å¼•æ·»åŠ æ–°æ–‡æ¡£
        
        Args:
            file_paths: æ–°æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            æ·»åŠ æ˜¯å¦æˆåŠŸ
        """
        if not self.is_initialized:
            print("æ£€ç´¢ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆæ„å»ºæˆ–åŠ è½½ç´¢å¼•")
            return False
        
        try:
            new_documents = self.document_loader.process_multiple_files(file_paths)
            if new_documents:
                self.vector_store.add_documents(new_documents)
                print(f"æˆåŠŸæ·»åŠ  {len(new_documents)} ä¸ªæ–°æ–‡æ¡£ç‰‡æ®µ")
                
                # å¦‚æœæœ‰æŒä¹…åŒ–è·¯å¾„ï¼Œä¿å­˜æ›´æ–°åçš„ç´¢å¼•
                if self.vector_store.persist_path:
                    self.vector_store.save()
                    print("ç´¢å¼•å·²æ›´æ–°å¹¶ä¿å­˜")
                
                return True
            else:
                print("æ²¡æœ‰æˆåŠŸåŠ è½½æ–°æ–‡æ¡£")
                return False
                
        except Exception as e:
            print(f"æ·»åŠ æ–‡æ¡£æ—¶å‡ºé”™: {e}")
            return False
    
    def retrieve(self, 
                query: str, 
                k: int = 5,
                use_rerank: bool = True,
                rerank_top_k: int = 10,
                return_scores: bool = False) -> List[Document]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›ç»“æœæ•°é‡
            use_rerank: æ˜¯å¦ä½¿ç”¨é‡æ’åº
            rerank_top_k: é‡æ’åºå‰çš„å€™é€‰æ•°é‡
            return_scores: æ˜¯å¦è¿”å›åˆ†æ•°
            
        Returns:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        if not self.is_initialized:
            raise ValueError("æ£€ç´¢ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆæ„å»ºæˆ–åŠ è½½ç´¢å¼•")
        
        print(f"æ­£åœ¨æ£€ç´¢æŸ¥è¯¢: '{query}'")
        
        if return_scores and not use_rerank:
            # è¿”å›å¸¦åˆ†æ•°çš„ç»“æœ
            results = self.vector_store.similarity_search_with_score(query, k=k)
            print(f"æ£€ç´¢åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
            return results
        else:
            # ä½¿ç”¨å‘é‡å­˜å‚¨çš„æ£€ç´¢æ–¹æ³•ï¼ˆå†…ç½®é‡æ’åºé€»è¾‘ï¼‰
            results = self.vector_store.similarity_search(
                query=query,
                k=k,
                use_rerank=use_rerank,
                rerank_top_k=rerank_top_k
            )
            print(f"æ£€ç´¢åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
            return results
    
    def batch_retrieve(self, 
                      queries: List[str], 
                      k: int = 5,
                      use_rerank: bool = True) -> Dict[str, List[Document]]:
        """
        æ‰¹é‡æ£€ç´¢
        
        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
            k: æ¯ä¸ªæŸ¥è¯¢è¿”å›çš„ç»“æœæ•°é‡
            use_rerank: æ˜¯å¦ä½¿ç”¨é‡æ’åº
            
        Returns:
            æŸ¥è¯¢åˆ°æ–‡æ¡£åˆ—è¡¨çš„æ˜ å°„
        """
        results = {}
        
        for query in queries:
            try:
                results[query] = self.retrieve(
                    query=query, 
                    k=k, 
                    use_rerank=use_rerank
                )
            except Exception as e:
                print(f"æŸ¥è¯¢ '{query}' å¤±è´¥: {e}")
                results[query] = []
        
        return results
    
    def get_document_by_metadata(self, 
                               metadata_filter: Dict[str, Any]) -> List[Document]:
        """
        æ ¹æ®å…ƒæ•°æ®ç­›é€‰æ–‡æ¡£ï¼ˆç®€å•å®ç°ï¼‰
        
        Args:
            metadata_filter: å…ƒæ•°æ®ç­›é€‰æ¡ä»¶
            
        Returns:
            åŒ¹é…çš„æ–‡æ¡£åˆ—è¡¨
        """
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°ï¼Œå®é™…ä½¿ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„ç­›é€‰é€»è¾‘
        # æˆ–è€…ä½¿ç”¨æ”¯æŒå…ƒæ•°æ®ç­›é€‰çš„å‘é‡æ•°æ®åº“
        print("å…ƒæ•°æ®ç­›é€‰åŠŸèƒ½éœ€è¦å‘é‡æ•°æ®åº“æ”¯æŒï¼Œå½“å‰ä¸ºç®€åŒ–å®ç°")
        return []
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–æ£€ç´¢ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        vector_stats = self.vector_store.get_stats()
        
        loader_stats = {
            "chunk_size": self.document_loader.chunk_size,
            "chunk_overlap": self.document_loader.chunk_overlap,
            "supported_formats": self.document_loader.get_supported_formats()
        }
        
        return {
            "system_status": "å·²åˆå§‹åŒ–" if self.is_initialized else "æœªåˆå§‹åŒ–",
            "vector_store": vector_stats,
            "document_loader": loader_stats
        }
    
    def save_index(self, path: Optional[str] = None):
        """ä¿å­˜ç´¢å¼•"""
        if not self.is_initialized:
            raise ValueError("æ£€ç´¢ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜")
        
        self.vector_store.save(path)
    
    def update_chunk_settings(self, chunk_size: int = None, chunk_overlap: int = None):
        """æ›´æ–°æ–‡æ¡£åˆ†å‰²è®¾ç½®"""
        self.document_loader.update_chunk_settings(chunk_size, chunk_overlap)


class MultiIndexRetrieval:
    """å¤šç´¢å¼•æ£€ç´¢ç³»ç»Ÿ - æ”¯æŒå¤šä¸ªç‹¬ç«‹çš„å‘é‡å­˜å‚¨"""
    
    def __init__(self):
        self.retrievers: Dict[str, RetrievalSystem] = {}
    
    def add_retriever(self, name: str, retriever: RetrievalSystem):
        """æ·»åŠ æ£€ç´¢å™¨"""
        self.retrievers[name] = retriever
    
    def create_retriever(self, 
                        name: str,
                        file_paths: List[str],
                        embedding_type: str = "siliconflow",  # é»˜è®¤ä½¿ç”¨siliconflow
                        model_name: str = "BAAI/bge-large-zh-v1.5",  # ä½¿ç”¨SiliconFlowæ¨èæ¨¡å‹
                        persist_path: Optional[str] = None,
                        chunk_size: int = 500,  # æ·»åŠ chunk_sizeå‚æ•°
                        chunk_overlap: int = 100,  # æ·»åŠ chunk_overlapå‚æ•°
                        **embedding_kwargs) -> bool:  # æ·»åŠ è¿™ä¸ªå‚æ•°
        """åˆ›å»ºæ–°çš„æ£€ç´¢å™¨"""
        retriever = RetrievalSystem(
            embedding_type=embedding_type,
            model_name=model_name,
            persist_path=persist_path,
            chunk_size=chunk_size,  # ä¼ é€’chunk_size
            chunk_overlap=chunk_overlap,  # ä¼ é€’chunk_overlap
            **embedding_kwargs  # ä¼ é€’é¢å¤–å‚æ•°
        )
        
        success = retriever.build_index_from_files(file_paths)
        if success:
            self.retrievers[name] = retriever
            print(f"æ£€ç´¢å™¨ '{name}' åˆ›å»ºæˆåŠŸ")
            return True
        else:
            print(f"æ£€ç´¢å™¨ '{name}' åˆ›å»ºå¤±è´¥")
            return False
    
    def retrieve_from_all(self, 
                         query: str, 
                         k_per_retriever: int = 3,
                         total_k: int = 5,
                         use_rerank: bool = True) -> List[Document]:
        """ä»æ‰€æœ‰æ£€ç´¢å™¨ä¸­æ£€ç´¢å¹¶åˆå¹¶ç»“æœ"""
        all_results = []
        
        for name, retriever in self.retrievers.items():
            try:
                results = retriever.retrieve(
                    query=query, 
                    k=k_per_retriever, 
                    use_rerank=False  # å…ˆä¸ç”¨é‡æ’åºï¼Œæœ€åç»Ÿä¸€é‡æ’
                )
                all_results.extend(results)
            except Exception as e:
                print(f"ä»æ£€ç´¢å™¨ '{name}' æ£€ç´¢å¤±è´¥: {e}")
        
        # å¦‚æœä½¿ç”¨é‡æ’åºï¼Œå¯¹æ‰€æœ‰ç»“æœç»Ÿä¸€é‡æ’
        if use_rerank and all_results:
            docs_contents = [doc.page_content for doc in all_results]
            reranked_results = rerank_base(query, docs_contents)
            
            # é‡æ–°ç»„ç»‡æ–‡æ¡£
            final_results = []
            for content, score in reranked_results[:total_k]:
                for doc in all_results:
                    if doc.page_content == content:
                        final_results.append(doc)
                        break
            
            return final_results
        
        return all_results[:total_k]
    
    def get_retriever_names(self) -> List[str]:
        """è·å–æ‰€æœ‰æ£€ç´¢å™¨åç§°"""
        return list(self.retrievers.keys())
    
    def get_retriever(self, name: str) -> Optional[RetrievalSystem]:
        """è·å–æŒ‡å®šæ£€ç´¢å™¨"""
        return self.retrievers.get(name)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import os
    
    print("RAGæ£€ç´¢ç³»ç»Ÿæµ‹è¯•")
    
    # æ£€æŸ¥APIå¯†é’¥
    try:
        from dotenv import load_dotenv
        load_dotenv()
    except ImportError:
        print("æç¤º: å»ºè®®å®‰è£… python-dotenv åŒ…")
    
    SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY", "your_default_key")
    
    if SILICONFLOW_API_KEY == "your_default_key":
        print("âŒ SiliconFlow APIå¯†é’¥æœªè®¾ç½®ï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®SILICONFLOW_API_KEY")
        print("åˆ›å»º.envæ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹:")
        print("SILICONFLOW_API_KEY=sk-your-actual-api-key")
    else:
        print("âœ… SiliconFlow APIå¯†é’¥å·²è®¾ç½®")
    
    # è·å–æµ‹è¯•æ–‡ä»¶è·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    test_file = os.path.join(current_dir, "pumpkin_book.pdf")
    
    if not os.path.exists(test_file):
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        print("è¯·ç¡®ä¿ pumpkin_book.pdf æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸‹")
        exit(1)
    
    print(f"âœ… æ‰¾åˆ°æµ‹è¯•æ–‡ä»¶: {os.path.basename(test_file)}")
    
    # ç¤ºä¾‹1: åŸºæœ¬ä½¿ç”¨
    print("\n=== åŸºæœ¬æ£€ç´¢ç³»ç»Ÿæµ‹è¯• ===")
    try:
        retrieval = RetrievalSystem(
            embedding_type="siliconflow",
            model_name="BAAI/bge-large-zh-v1.5",
            persist_path="./test_retrieval_index",
            chunk_size=500,  # ä½¿ç”¨æ›´å°çš„chunk size
            chunk_overlap=50,  # ç›¸åº”å‡å°‘overlap
            api_key=SILICONFLOW_API_KEY
        )
        print("âœ… æ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        
        # å®é™…æ„å»ºç´¢å¼•
        print(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {os.path.basename(test_file)}")
        success = retrieval.build_index_from_files([test_file])
        
        if success:
            print("âœ… ç´¢å¼•æ„å»ºæˆåŠŸ")
            
            # æµ‹è¯•æŸ¥è¯¢
            test_queries = [
                "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
                "æ·±åº¦å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ",
                "ç¥ç»ç½‘ç»œå¦‚ä½•å·¥ä½œï¼Ÿ"
            ]
            
            for query in test_queries:
                print(f"\nğŸ” æŸ¥è¯¢: {query}")
                try:
                    results = retrieval.retrieve(query, k=3, use_rerank=True)
                    print(f"âœ… æ£€ç´¢åˆ° {len(results)} ä¸ªç»“æœ")
                    
                    for i, doc in enumerate(results):
                        content_preview = doc.page_content[:100].replace('\n', ' ')
                        source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')
                        print(f"  {i+1}. {content_preview}...")
                        print(f"     æ¥æº: {source}")
                
                except Exception as e:
                    print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
        
        else:
            print("âŒ ç´¢å¼•æ„å»ºå¤±è´¥")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = retrieval.get_stats()
        print(f"\nğŸ“Š ç³»ç»Ÿç»Ÿè®¡:")
        print(f"  ç³»ç»ŸçŠ¶æ€: {stats['system_status']}")
        print(f"  å‘é‡å­˜å‚¨: {stats['vector_store']}")
        print(f"  æ–‡æ¡£å¤„ç†: chunk_size={stats['document_loader']['chunk_size']}, "
              f"chunk_overlap={stats['document_loader']['chunk_overlap']}")
        
    except Exception as e:
        print(f"âŒ åŸºæœ¬æ£€ç´¢ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
    
    print("\n=== å¤šç´¢å¼•æ£€ç´¢ç³»ç»Ÿæµ‹è¯• ===")
    try:
        multi_retrieval = MultiIndexRetrieval()
        print("âœ… å¤šç´¢å¼•æ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        
        # åˆ›å»ºæ£€ç´¢å™¨ï¼ˆä½¿ç”¨åŒä¸€ä¸ªæ–‡ä»¶ï¼Œä½†å¯ä»¥æ¨¡æ‹Ÿä¸åŒçš„é…ç½®ï¼‰
        success1 = multi_retrieval.create_retriever(
            name="å—ç“œä¹¦-å®Œæ•´ç‰ˆ", 
            file_paths=[test_file],
            embedding_type="siliconflow",
            model_name="BAAI/bge-large-zh-v1.5",
            chunk_size=500,  # ä½¿ç”¨æ›´å°çš„chunk size
            chunk_overlap=50,  # ç›¸åº”å‡å°‘overlap
            api_key=SILICONFLOW_API_KEY,
            persist_path="./multi_index_1"
        )
        
        if success1:
            print("âœ… å¤šç´¢å¼•æ£€ç´¢å™¨åˆ›å»ºæˆåŠŸ")
            
            # è·¨ç´¢å¼•æ£€ç´¢æµ‹è¯•
            test_query = "æœºå™¨å­¦ä¹ çš„åŸºæœ¬åŸç†"
            print(f"\nğŸ” è·¨ç´¢å¼•æŸ¥è¯¢: {test_query}")
            
            try:
                combined_results = multi_retrieval.retrieve_from_all(
                    query=test_query, 
                    k_per_retriever=2,
                    total_k=3,
                    use_rerank=True
                )
                print(f"âœ… è·¨ç´¢å¼•æ£€ç´¢æˆåŠŸï¼Œå…± {len(combined_results)} ä¸ªç»“æœ")
                
                for i, doc in enumerate(combined_results):
                    content_preview = doc.page_content[:80].replace('\n', ' ')
                    print(f"  {i+1}. {content_preview}...")
            
            except Exception as e:
                print(f"âŒ è·¨ç´¢å¼•æ£€ç´¢å¤±è´¥: {e}")
        
        else:
            print("âŒ å¤šç´¢å¼•æ£€ç´¢å™¨åˆ›å»ºå¤±è´¥")
        
        retriever_names = multi_retrieval.get_retriever_names()
        print(f"ğŸ“‹ å½“å‰æ£€ç´¢å™¨åˆ—è¡¨: {retriever_names}")
        
    except Exception as e:
        print(f"âŒ å¤šç´¢å¼•æ£€ç´¢ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
    
    print("\nğŸ‰ æ£€ç´¢ç³»ç»Ÿæµ‹è¯•å®Œæˆ")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
    print("\nğŸ§¹ æ¸…ç†æµ‹è¯•ç´¢å¼•æ–‡ä»¶...")
    try:
        import shutil
        test_dirs = ["./test_retrieval_index", "./multi_index_1"]
        for test_dir in test_dirs:
            if os.path.exists(test_dir):
                shutil.rmtree(test_dir)
                print(f"âœ… å·²åˆ é™¤: {test_dir}")
    except Exception as e:
        print(f"âš ï¸ æ¸…ç†å¤±è´¥: {e} (å¯æ‰‹åŠ¨åˆ é™¤æµ‹è¯•ç›®å½•)")