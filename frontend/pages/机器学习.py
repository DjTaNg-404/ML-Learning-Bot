import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn import datasets
from sklearn.model_selection import train_test_split, learning_curve, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve, auc,
    mean_squared_error, mean_absolute_error, r2_score
)
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.svm import SVC, SVR
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.ensemble import (
    RandomForestClassifier, RandomForestRegressor,
    GradientBoostingClassifier, GradientBoostingRegressor
)
from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso
import xgboost as xgb
import time
import uuid
from io import BytesIO, StringIO
from PIL import Image
import io
import base64
import joblib
import os
from tempfile import NamedTemporaryFile

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒï¼ˆè§£å†³ä¹±ç é—®é¢˜ï¼‰
plt.rcParams["font.family"] = ["Microsoft YaHei", "SimSun", "SimHei", "KaiTi", "Arial Unicode MS"]
plt.rcParams["axes.unicode_minus"] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
sns.set(font_scale=1.2)
sns.set_style("whitegrid")
sns.set(font="Microsoft YaHei")  # ä¸ºseabornå•ç‹¬è®¾ç½®å­—ä½“

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="æœºå™¨å­¦ä¹ å¯è§†åŒ–å¹³å°",
    page_icon="ğŸ“Š",
    layout="wide"
)

# ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨
if not os.path.exists("temp"):
    os.makedirs("temp")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
def init_session_state():
    if "session_id" not in st.session_state:
        st.session_state.session_id = str(uuid.uuid4())
    
    if "dataset" not in st.session_state:
        st.session_state.dataset = None
    
    if "dataset_type" not in st.session_state:
        st.session_state.dataset_type = "built_in"  # built_in æˆ– custom
    
    if "task_type" not in st.session_state:
        st.session_state.task_type = "classification"  # classification æˆ– regression
    
    if "raw_data" not in st.session_state:
        st.session_state.raw_data = None
    
    if "processed_data" not in st.session_state:
        st.session_state.processed_data = None
    
    if "target_column" not in st.session_state:
        st.session_state.target_column = None
    
    if "model" not in st.session_state:
        st.session_state.model = None
    
    if "model_name" not in st.session_state:
        st.session_state.model_name = "æœªé€‰æ‹©æ¨¡å‹"
    
    if "train_metrics" not in st.session_state:
        st.session_state.train_metrics = {}
    
    if "test_metrics" not in st.session_state:
        st.session_state.test_metrics = {}
    
    if "is_trained" not in st.session_state:
        st.session_state.is_trained = False
    
    if "X_train" not in st.session_state:
        st.session_state.X_train = None
    
    if "X_test" not in st.session_state:
        st.session_state.X_test = None
    
    if "y_train" not in st.session_state:
        st.session_state.y_train = None
    
    if "y_test" not in st.session_state:
        st.session_state.y_test = None
    
    if "y_pred" not in st.session_state:
        st.session_state.y_pred = None
    
    if "training_time" not in st.session_state:
        st.session_state.training_time = 0.0
    
    if "feature_names" not in st.session_state:
        st.session_state.feature_names = None
    
    if "target_names" not in st.session_state:
        st.session_state.target_names = None
    
    # å­˜å‚¨åŸå§‹æ ‡ç­¾ç¼–ç å™¨ï¼Œç”¨äºåç»­æ˜ å°„
    if "label_encoder" not in st.session_state:
        st.session_state.label_encoder = None
    
    # å­˜å‚¨å†å²è®­ç»ƒè®°å½•
    if "training_history" not in st.session_state:
        st.session_state.training_history = []
    
    # ç‰¹å¾å·¥ç¨‹ç›¸å…³
    if "categorical_cols" not in st.session_state:
        st.session_state.categorical_cols = []
    
    if "numerical_cols" not in st.session_state:
        st.session_state.numerical_cols = []

# åŠ è½½å†…ç½®æ•°æ®é›†
@st.cache_data
def load_builtin_dataset(dataset_name, task_type):
    # å­˜å‚¨æ•°æ®é›†å…ƒä¿¡æ¯ï¼Œç”¨äºåç»­è¯†åˆ«
    dataset_info = {
        "classification": {
            "é¸¢å°¾èŠ±æ•°æ®é›†": {"loader": datasets.load_iris, "name_key": "iris"},
            "ä¹³è…ºç™Œæ•°æ®é›†": {"loader": datasets.load_breast_cancer, "name_key": "breast_cancer"},
            "è‘¡è„é…’æ•°æ®é›†": {"loader": datasets.load_wine, "name_key": "wine"}
        },
        "regression": {
            "ç³–å°¿ç—…æ•°æ®é›†": {"loader": datasets.load_diabetes, "name_key": "diabetes"},
            "åŠ å·æˆ¿ä»·æ•°æ®é›†": {"loader": datasets.fetch_california_housing, "name_key": "california_housing"}
        }
    }
    
    if task_type not in dataset_info or dataset_name not in dataset_info[task_type]:
        return None, None, None, None, None, None, None  # è¿”å›å®Œæ•´Noneå…ƒç»„
    
    try:
        # åŠ è½½æ•°æ®é›†
        data = dataset_info[task_type][dataset_name]["loader"]()
        # å­˜å‚¨æ•°æ®é›†æ ‡è¯†ï¼Œç”¨äºåç»­è¯†åˆ«
        data["name_key"] = dataset_info[task_type][dataset_name]["name_key"]
        
        # ç‰¹æ®Šå¤„ç†åŠ å·æˆ¿ä»·æ•°æ®é›†ï¼Œå®ƒçš„dataå’Œtargetå±æ€§åç§°ä¸åŒ
        if data["name_key"] == "california_housing":
            X = data.data
            y = data.target
        else:
            X = data.data
            y = data.target
        
        # å¤„ç†ç‰¹å¾åç§°
        if hasattr(data, 'feature_names'):
            feature_names = data.feature_names
        else:
            feature_names = [f"ç‰¹å¾{i+1}" for i in range(X.shape[1])]
        
        # å¤„ç†ç›®æ ‡åç§°
        target_names = None
        if task_type == "classification" and hasattr(data, 'target_names'):
            target_names = data.target_names
        
        # åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )
        
        # æ•°æ®æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)
        
        return X_train, X_test, y_train, y_test, feature_names, target_names, data
    except Exception as e:
        st.error(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {str(e)}")
        return None, None, None, None, None, None, None

# åŠ è½½ç”¨æˆ·ä¸Šä¼ çš„æ•°æ®é›†
@st.cache_data
def load_custom_dataset(file, file_type):
    try:
        if file_type == "csv":
            # é™åˆ¶æ–‡ä»¶å¤§å°ï¼Œé˜²æ­¢è¿‡å¤§æ–‡ä»¶
            if file.size > 200 * 1024 * 1024:  # 200MB
                return None, "æ–‡ä»¶è¿‡å¤§ï¼Œè¯·ä¸Šä¼ å°äº200MBçš„æ–‡ä»¶"
            
            # å¯¹äºå¤§å‹CSVï¼Œå…ˆè¯»å–éƒ¨åˆ†è¡Œè¿›è¡Œé¢„è§ˆ
            if file.size > 10 * 1024 * 1024:  # 10MB
                file.seek(0)
                df = pd.read_csv(file, nrows=10000)  # åªè¯»å–10000è¡Œ
                return df, "æˆåŠŸåŠ è½½æ•°æ®é¢„è§ˆï¼ˆä»…å‰10000è¡Œï¼‰"
            else:
                file.seek(0)
                df = pd.read_csv(file)
        elif file_type in ["xls", "xlsx"]:
            file.seek(0)
            df = pd.read_excel(file)
        else:
            return None, "ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹"
        
        return df, "æˆåŠŸåŠ è½½æ•°æ®"
    except Exception as e:
        return None, f"åŠ è½½å¤±è´¥: {str(e)}"

# æ•°æ®æ¸…æ´—å¤„ç†
def clean_data(df, missing_value_strategy, categorical_encoding, target_col=None, task_type="classification"):
    # å¤åˆ¶æ•°æ®é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
    cleaned_df = df.copy()
    
    # è¯†åˆ«æ•°å€¼å‹å’Œç±»åˆ«å‹ç‰¹å¾
    numerical_cols = cleaned_df.select_dtypes(include=['number']).columns.tolist()
    categorical_cols = cleaned_df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    # å¦‚æœæŒ‡å®šäº†ç›®æ ‡åˆ—ï¼Œä»ç‰¹å¾åˆ—è¡¨ä¸­ç§»é™¤
    if target_col and target_col in numerical_cols:
        numerical_cols.remove(target_col)
    if target_col and target_col in categorical_cols:
        categorical_cols.remove(target_col)
    
    # å¤„ç†ç¼ºå¤±å€¼
    if missing_value_strategy == "åˆ é™¤è¡Œ":
        cleaned_df = cleaned_df.dropna()
    elif missing_value_strategy == "å‡å€¼å¡«å……":
        # å¯¹æ•°å€¼ç‰¹å¾ä½¿ç”¨å‡å€¼å¡«å……
        cleaned_df[numerical_cols] = cleaned_df[numerical_cols].fillna(cleaned_df[numerical_cols].mean())
        # å¯¹ç±»åˆ«ç‰¹å¾ä½¿ç”¨ä¼—æ•°å¡«å……
        for col in categorical_cols:
            cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].mode().iloc[0] if not cleaned_df[col].mode().empty else "")
    elif missing_value_strategy == "ä¸­ä½æ•°å¡«å……":
        # å¯¹æ•°å€¼ç‰¹å¾ä½¿ç”¨ä¸­ä½æ•°å¡«å……
        cleaned_df[numerical_cols] = cleaned_df[numerical_cols].fillna(cleaned_df[numerical_cols].median())
        # å¯¹ç±»åˆ«ç‰¹å¾ä½¿ç”¨ä¼—æ•°å¡«å……
        for col in categorical_cols:
            cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].mode().iloc[0] if not cleaned_df[col].mode().empty else "")
    
    # å­˜å‚¨æ ‡ç­¾ç¼–ç å™¨å’ŒåŸå§‹ç±»åˆ«åç§°
    label_encoder = None
    target_names = None
    
    # å¦‚æœæŒ‡å®šäº†ç›®æ ‡åˆ—ï¼Œå¤„ç†ç›®æ ‡åˆ—çš„ç¼–ç ï¼ˆä»…åˆ†ç±»ä»»åŠ¡ï¼‰
    if task_type == "classification" and target_col and target_col in cleaned_df.columns:
        # æ£€æŸ¥ç›®æ ‡åˆ—æ˜¯å¦ä¸ºç±»åˆ«å‹
        if cleaned_df[target_col].dtype in ['object', 'category']:
            # å¯¹ç›®æ ‡åˆ—è¿›è¡Œç¼–ç 
            label_encoder = LabelEncoder()
            cleaned_df[target_col] = label_encoder.fit_transform(cleaned_df[target_col])
            target_names = label_encoder.classes_  # åŸå§‹ç±»åˆ«åç§°
    
    # å¯¹å…¶ä»–ç±»åˆ«ç‰¹å¾è¿›è¡Œç¼–ç 
    for col in categorical_cols:
        if col != target_col:  # å·²å¤„ç†è¿‡ç›®æ ‡åˆ—
            if categorical_encoding == "æ ‡ç­¾ç¼–ç ":
                le = LabelEncoder()
                cleaned_df[col] = le.fit_transform(cleaned_df[col].astype(str))
            elif categorical_encoding == "ç‹¬çƒ­ç¼–ç ":
                # ä½¿ç”¨ç‹¬çƒ­ç¼–ç 
                dummies = pd.get_dummies(cleaned_df[col], prefix=col, drop_first=True)
                cleaned_df = pd.concat([cleaned_df, dummies], axis=1)
                cleaned_df.drop(col, axis=1, inplace=True)
    
    return cleaned_df, target_names, label_encoder, numerical_cols, categorical_cols

# å‡†å¤‡è®­ç»ƒæ•°æ®
def prepare_training_data(df, target_col, test_size=0.3, task_type="classification"):
    if target_col not in df.columns:
        return None, None, None, None, None, None, "ç›®æ ‡åˆ—ä¸å­˜åœ¨"
    
    # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
    X = df.drop(columns=[target_col]).values
    y = df[target_col].values
    
    # åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42
    )
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    
    feature_names = df.drop(columns=[target_col]).columns.tolist()
    
    # è·å–å¤„ç†åæ•°æ®ä¸­å®é™…å­˜åœ¨çš„ç±»åˆ«ï¼ˆä»…åˆ†ç±»ä»»åŠ¡ï¼‰
    unique_classes = None
    if task_type == "classification":
        unique_classes = np.unique(y)
    
    return X_train, X_test, y_train, y_test, feature_names, unique_classes, "æ•°æ®å‡†å¤‡å®Œæˆ"

# åˆ›å»ºæ¨¡å‹
def create_model(model_name, params, task_type="classification"):
    try:
        if task_type == "classification":
            if model_name == "Kè¿‘é‚»åˆ†ç±»å™¨":
                return KNeighborsClassifier(n_neighbors=params["n_neighbors"])
            elif model_name == "æ”¯æŒå‘é‡æœº":
                return SVC(C=params["C"], kernel=params["kernel"], probability=True, random_state=42)
            elif model_name == "å†³ç­–æ ‘":
                return DecisionTreeClassifier(max_depth=params["max_depth"], random_state=42)
            elif model_name == "éšæœºæ£®æ—":
                return RandomForestClassifier(n_estimators=params["n_estimators"], random_state=42)
            elif model_name == "é€»è¾‘å›å½’":
                return LogisticRegression(C=params["C"], max_iter=1000, random_state=42)
            elif model_name == "æ¢¯åº¦æå‡æ ‘":
                return GradientBoostingClassifier(n_estimators=params["n_estimators"], random_state=42)
            elif model_name == "XGBoost":
                return xgb.XGBClassifier(n_estimators=params["n_estimators"], random_state=42)
        
        elif task_type == "regression":
            if model_name == "Kè¿‘é‚»å›å½’å™¨":
                return KNeighborsRegressor(n_neighbors=params["n_neighbors"])
            elif model_name == "æ”¯æŒå‘é‡å›å½’":
                return SVR(C=params["C"], kernel=params["kernel"])
            elif model_name == "å†³ç­–æ ‘å›å½’":
                return DecisionTreeRegressor(max_depth=params["max_depth"], random_state=42)
            elif model_name == "éšæœºæ£®æ—å›å½’":
                return RandomForestRegressor(n_estimators=params["n_estimators"], random_state=42)
            elif model_name == "çº¿æ€§å›å½’":
                return LinearRegression()
            elif model_name == "å²­å›å½’":
                return Ridge(alpha=params["alpha"])
            elif model_name == "Lassoå›å½’":
                return Lasso(alpha=params["alpha"], max_iter=1000)
            elif model_name == "æ¢¯åº¦æå‡å›å½’":
                return GradientBoostingRegressor(n_estimators=params["n_estimators"], random_state=42)
            elif model_name == "XGBoostå›å½’":
                return xgb.XGBRegressor(n_estimators=params["n_estimators"], random_state=42)
        
        return None
    except Exception as e:
        st.error(f"åˆ›å»ºæ¨¡å‹å¤±è´¥: {str(e)}")
        return None

# è¶…å‚æ•°è°ƒä¼˜
def tune_model(model, param_grid, X_train, y_train, cv=3):
    try:
        grid_search = GridSearchCV(
            estimator=model,
            param_grid=param_grid,
            cv=cv,
            scoring='accuracy' if st.session_state.task_type == "classification" else 'neg_mean_squared_error',
            n_jobs=-1,
            verbose=0
        )
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ä»¥æ˜¾ç¤ºè¿›åº¦
        for i in range(1, 101):
            time.sleep(0.01)  # æ¨¡æ‹Ÿè¿›åº¦
            progress_bar.progress(i)
            status_text.text(f"è¶…å‚æ•°æœç´¢è¿›åº¦: {i}%")
            
            # å®é™…è®­ç»ƒåªæ‰§è¡Œä¸€æ¬¡
            if i == 1:
                grid_search.fit(X_train, y_train)
        
        status_text.text("è¶…å‚æ•°æœç´¢å®Œæˆ!")
        return grid_search.best_estimator_, grid_search.best_params_
    except Exception as e:
        st.error(f"è¶…å‚æ•°è°ƒä¼˜å¤±è´¥: {str(e)}")
        return model, {}

# è®­ç»ƒæ¨¡å‹
def train_model(model, X_train, y_train):
    try:
        start_time = time.time()
        
        # æ·»åŠ è¿›åº¦æ¡
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ä»¥æ˜¾ç¤ºè¿›åº¦ï¼ˆå®é™…è®­ç»ƒåªæ‰§è¡Œä¸€æ¬¡ï¼‰
        for i in range(1, 101):
            time.sleep(0.01)  # æ¨¡æ‹Ÿè¿›åº¦
            progress_bar.progress(i)
            status_text.text(f"æ¨¡å‹è®­ç»ƒè¿›åº¦: {i}%")
            
            # å®é™…è®­ç»ƒåªæ‰§è¡Œä¸€æ¬¡
            if i == 1:
                model.fit(X_train, y_train)
        
        training_time = time.time() - start_time
        status_text.text(f"è®­ç»ƒå®Œæˆ! è€—æ—¶: {training_time:.4f}ç§’")
        return model, training_time, None
    except ValueError as e:
        return None, 0, f"æ•°æ®æ ¼å¼é”™è¯¯: {str(e)}ï¼Œè¯·æ£€æŸ¥ç‰¹å¾æ˜¯å¦åŒ…å«éæ•°å€¼ç±»å‹"
    except Exception as e:
        return None, 0, f"è®­ç»ƒå¤±è´¥: {str(e)}"

# è¯„ä¼°æ¨¡å‹
def evaluate_model(model, X, y, task_type="classification", target_names=None):
    try:
        y_pred = model.predict(X)
        
        if task_type == "classification":
            accuracy = accuracy_score(y, y_pred)
            precision = precision_score(y, y_pred, average='weighted')
            recall = recall_score(y, y_pred, average='weighted')
            f1 = f1_score(y, y_pred, average='weighted')
            
            return {
                "accuracy": accuracy,
                "precision": precision,
                "recall": recall,
                "f1": f1
            }, y_pred
        else:  # regression
            mse = mean_squared_error(y, y_pred)
            rmse = np.sqrt(mse)
            mae = mean_absolute_error(y, y_pred)
            r2 = r2_score(y, y_pred)
            
            return {
                "mse": mse,
                "rmse": rmse,
                "mae": mae,
                "r2": r2
            }, y_pred
    except Exception as e:
        st.error(f"æ¨¡å‹è¯„ä¼°å¤±è´¥: {str(e)}")
        return {}, None

# ä¿å­˜æ¨¡å‹
def save_model(model, model_name):
    try:
        with NamedTemporaryFile(delete=False, suffix='.pkl', dir='temp') as tmp_file:
            joblib.dump(model, tmp_file)
            return tmp_file.name
    except Exception as e:
        st.error(f"ä¿å­˜æ¨¡å‹å¤±è´¥: {str(e)}")
        return None

# åŠ è½½æ¨¡å‹
def load_model(file):
    try:
        return joblib.load(file)
    except Exception as e:
        st.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")
        return None

# ç»˜åˆ¶æ··æ·†çŸ©é˜µï¼ˆäº¤äº’å¼ï¼‰
def plot_confusion_matrix(y_true, y_pred, class_names):
    cm = confusion_matrix(y_true, y_pred)
    
    fig = px.imshow(
        cm, 
        labels=dict(x="é¢„æµ‹æ ‡ç­¾", y="çœŸå®æ ‡ç­¾", color="æ•°é‡"),
        x=class_names,
        y=class_names,
        text_auto=True,
        color_continuous_scale="Blues"
    )
    fig.update_layout(
        title="æ··æ·†çŸ©é˜µ",
        width=700,
        height=600,
    )
    return fig

# ç»˜åˆ¶å­¦ä¹ æ›²çº¿ï¼ˆäº¤äº’å¼ï¼‰
def plot_learning_curve(estimator, X, y, title):
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, train_sizes=np.linspace(0.1, 1.0, 10),
        cv=5, 
        scoring='accuracy' if st.session_state.task_type == "classification" else 'neg_mean_squared_error',
        n_jobs=-1, 
        random_state=42
    )
    
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    test_mean = np.mean(test_scores, axis=1)
    test_std = np.std(test_scores, axis=1)
    
    # å¯¹äºå›å½’ä»»åŠ¡ï¼Œè½¬æ¢ä¸ºæ­£æ•°ä»¥ä¾¿å±•ç¤º
    if st.session_state.task_type == "regression":
        train_mean = -train_mean
        test_mean = -test_mean
    
    fig = go.Figure()
    
    # æ·»åŠ è®­ç»ƒå‡†ç¡®ç‡æ›²çº¿
    fig.add_trace(go.Scatter(
        x=train_sizes, y=train_mean,
        mode='lines+markers',
        name='è®­ç»ƒåˆ†æ•°',
        line=dict(color='blue'),
        marker=dict(symbol='circle')
    ))
    
    # æ·»åŠ è®­ç»ƒå‡†ç¡®ç‡è¯¯å·®å¸¦
    fig.add_trace(go.Scatter(
        x=train_sizes, y=train_mean + train_std,
        mode='lines',
        line=dict(width=0),
        showlegend=False
    ))
    fig.add_trace(go.Scatter(
        x=train_sizes, y=train_mean - train_std,
        mode='lines',
        fill='tonexty',
        fillcolor='rgba(0, 0, 255, 0.1)',
        line=dict(width=0),
        showlegend=False
    ))
    
    # æ·»åŠ éªŒè¯å‡†ç¡®ç‡æ›²çº¿
    fig.add_trace(go.Scatter(
        x=train_sizes, y=test_mean,
        mode='lines+markers',
        name='éªŒè¯åˆ†æ•°',
        line=dict(color='green'),
        marker=dict(symbol='square')
    ))
    
    # æ·»åŠ éªŒè¯å‡†ç¡®ç‡è¯¯å·®å¸¦
    fig.add_trace(go.Scatter(
        x=train_sizes, y=test_mean + test_std,
        mode='lines',
        line=dict(width=0),
        showlegend=False
    ))
    fig.add_trace(go.Scatter(
        x=train_sizes, y=test_mean - test_std,
        mode='lines',
        fill='tonexty',
        fillcolor='rgba(0, 255, 0, 0.1)',
        line=dict(width=0),
        showlegend=False
    ))
    
    fig.update_layout(
        title=title,
        xaxis_title='è®­ç»ƒæ ·æœ¬æ•°',
        yaxis_title='å‡†ç¡®ç‡' if st.session_state.task_type == "classification" else 'è´Ÿå‡æ–¹è¯¯å·®',
        width=800,
        height=500,
        legend=dict(
            yanchor="top",
            y=0.99,
            xanchor="right",
            x=0.99
        )
    )
    
    return fig

# ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§ï¼ˆäº¤äº’å¼ï¼‰
def plot_feature_importance(model, feature_names):
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
        indices = np.argsort(importances)[::-1]
        
        # åªæ˜¾ç¤ºå‰15ä¸ªç‰¹å¾ï¼Œé¿å…å¤ªå¤šç‰¹å¾å¯¼è‡´å›¾è¡¨æ‹¥æŒ¤
        if len(importances) > 15:
            indices = indices[:15]
            importances = importances[:15]
            feature_names = [feature_names[i] for i in indices]
        
        fig = px.bar(
            x=feature_names,
            y=importances,
            labels={'x': 'ç‰¹å¾', 'y': 'é‡è¦æ€§'},
            title='ç‰¹å¾é‡è¦æ€§',
            color=importances,
            color_continuous_scale='viridis'
        )
        fig.update_layout(
            width=800,
            height=500,
            xaxis_tickangle=-45
        )
        return fig
    return None

# ç»˜åˆ¶ROCæ›²çº¿ï¼ˆäº¤äº’å¼ï¼‰
def plot_roc_curve(model, X_test, y_test, class_names):
    if len(np.unique(y_test)) == 2:  # äºŒåˆ†ç±»é—®é¢˜
        y_score = model.predict_proba(X_test)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, y_score)
        roc_auc = auc(fpr, tpr)
        
        fig = go.Figure()
        
        # æ·»åŠ ROCæ›²çº¿
        fig.add_trace(go.Scatter(
            x=fpr, y=tpr,
            mode='lines',
            name=f'ROCæ›²çº¿ (é¢ç§¯ = {roc_auc:.2f})',
            line=dict(color='darkorange', width=2)
        ))
        
        # æ·»åŠ å¯¹è§’çº¿
        fig.add_trace(go.Scatter(
            x=[0, 1], y=[0, 1],
            mode='lines',
            name='éšæœºçŒœæµ‹',
            line=dict(color='navy', width=2, dash='dash')
        ))
        
        fig.update_layout(
            title='ROCæ›²çº¿',
            xaxis_title='å‡æ­£ä¾‹ç‡',
            yaxis_title='çœŸæ­£ä¾‹ç‡',
            xaxis=dict(range=[0.0, 1.0]),
            yaxis=dict(range=[0.0, 1.05]),
            width=800,
            height=500,
            legend=dict(
                yanchor="bottom",
                y=0.01,
                xanchor="right",
                x=0.99
            )
        )
        
        return fig
    return None

# ç»˜åˆ¶é¢„æµ‹å€¼ä¸çœŸå®å€¼å¯¹æ¯”å›¾ï¼ˆå›å½’ä»»åŠ¡ï¼‰
def plot_prediction_vs_actual(y_true, y_pred):
    fig = px.scatter(
        x=y_true,
        y=y_pred,
        labels={'x': 'çœŸå®å€¼', 'y': 'é¢„æµ‹å€¼'},
        title='é¢„æµ‹å€¼ vs çœŸå®å€¼'
    )
    
    # æ·»åŠ ç†æƒ³çº¿ï¼ˆy=xï¼‰
    min_val = min(min(y_true), min(y_pred))
    max_val = max(max(y_true), max(y_pred))
    fig.add_trace(go.Scatter(
        x=[min_val, max_val],
        y=[min_val, max_val],
        mode='lines',
        name='ç†æƒ³é¢„æµ‹',
        line=dict(color='red', dash='dash')
    ))
    
    fig.update_layout(
        width=800,
        height=500,
        legend=dict(
            yanchor="top",
            y=0.99,
            xanchor="right",
            x=0.99
        )
    )
    return fig

# ç»˜åˆ¶æ®‹å·®å›¾ï¼ˆå›å½’ä»»åŠ¡ï¼‰
def plot_residuals(y_true, y_pred):
    residuals = y_true - y_pred
    
    fig = px.scatter(
        x=y_pred,
        y=residuals,
        labels={'x': 'é¢„æµ‹å€¼', 'y': 'æ®‹å·® (çœŸå®å€¼-é¢„æµ‹å€¼)'},
        title='æ®‹å·®å›¾'
    )
    
    # æ·»åŠ æ°´å¹³çº¿y=0
    fig.add_hline(y=0, line_dash="dash", line_color="red")
    
    fig.update_layout(
        width=800,
        height=500
    )
    return fig

# æ•°æ®é›†ä¸‹è½½åŠŸèƒ½
def get_dataset_download_link(df, filename, text):
    """ç”Ÿæˆæ•°æ®é›†ä¸‹è½½é“¾æ¥"""
    try:
        # æ ¹æ®æ–‡ä»¶ç±»å‹è½¬æ¢æ•°æ®
        if filename.endswith('.csv'):
            # è½¬æ¢ä¸ºCSVå­—ç¬¦ä¸²
            csv = df.to_csv(index=False)
            b = csv.encode()
        elif filename.endswith('.xlsx'):
            # è½¬æ¢ä¸ºExcel
            output = BytesIO()
            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                df.to_excel(writer, index=False, sheet_name='æ•°æ®é›†')
            b = output.getvalue()
        
        # ç”Ÿæˆä¸‹è½½é“¾æ¥
        href = f'<a href="data:file/csv;base64,{base64.b64encode(b).decode()}" download="{filename}">{text}</a>'
        return href
    except Exception as e:
        st.error(f"ç”Ÿæˆä¸‹è½½é“¾æ¥å¤±è´¥: {str(e)}")
        return None

# ä¸»å‡½æ•°
def main():
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    init_session_state()
    
    # é¡µé¢æ ‡é¢˜
    st.title("ğŸ“Š æœºå™¨å­¦ä¹ å¯è§†åŒ–å¹³å°")
    st.markdown("---")
    
    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("ğŸ”§ é…ç½®é¢æ¿")
        
        # ä¼šè¯ä¿¡æ¯
        st.subheader("ä¼šè¯ä¿¡æ¯")
        st.write(f"**ä¼šè¯ID:** `{st.session_state.session_id[:8]}...`")
        
        st.markdown("---")
        
        # ä»»åŠ¡ç±»å‹é€‰æ‹©
        st.subheader("1. ä»»åŠ¡ç±»å‹")
        task_type = st.radio(
            "é€‰æ‹©æœºå™¨å­¦ä¹ ä»»åŠ¡ç±»å‹",
            ["åˆ†ç±»ä»»åŠ¡", "å›å½’ä»»åŠ¡"],
            index=0 if st.session_state.task_type == "classification" else 1
        )
        
        # æ›´æ–°ä»»åŠ¡ç±»å‹çŠ¶æ€
        st.session_state.task_type = "classification" if task_type == "åˆ†ç±»ä»»åŠ¡" else "regression"
        
        st.markdown("---")
        
        # æ•°æ®é›†æ¥æºé€‰æ‹©
        st.subheader("2. æ•°æ®é›†æ¥æº")
        dataset_source = st.radio(
            "é€‰æ‹©æ•°æ®é›†ç±»å‹",
            ["å†…ç½®æ•°æ®é›†", "è‡ªå®šä¹‰æ•°æ®é›†ä¸Šä¼ "],
            index=0 if st.session_state.dataset_type == "built_in" else 1
        )
        
        # æ›´æ–°æ•°æ®é›†ç±»å‹çŠ¶æ€
        if dataset_source == "å†…ç½®æ•°æ®é›†":
            st.session_state.dataset_type = "built_in"
        else:
            st.session_state.dataset_type = "custom"
        
        # å†…ç½®æ•°æ®é›†é€‰æ‹©
        if st.session_state.dataset_type == "built_in":
            dataset_options = ["é¸¢å°¾èŠ±æ•°æ®é›†", "ä¹³è…ºç™Œæ•°æ®é›†", "è‘¡è„é…’æ•°æ®é›†"] if st.session_state.task_type == "classification" else \
                             ["ç³–å°¿ç—…æ•°æ®é›†", "åŠ å·æˆ¿ä»·æ•°æ®é›†"]
            
            dataset_name = st.selectbox(
                "é€‰æ‹©ç¤ºä¾‹æ•°æ®é›†",
                dataset_options
            )
            
            # åŠ è½½æ•°æ®é›†æŒ‰é’®
            if st.button("åŠ è½½æ•°æ®é›†", use_container_width=True):
                with st.spinner("æ­£åœ¨åŠ è½½æ•°æ®é›†..."):
                    result = load_builtin_dataset(dataset_name, st.session_state.task_type)
                    if result[0] is not None:
                        X_train, X_test, y_train, y_test, feature_names, target_names, data = result
                        st.session_state.dataset = data
                        st.session_state.X_train = X_train
                        st.session_state.X_test = X_test
                        st.session_state.y_train = y_train
                        st.session_state.y_test = y_test
                        st.session_state.feature_names = feature_names
                        st.session_state.target_names = target_names
                        st.success(f"âœ… å·²æˆåŠŸåŠ è½½ {dataset_name}")
                        st.session_state.is_trained = False  # é‡æ–°åŠ è½½æ•°æ®é›†åé‡ç½®æ¨¡å‹çŠ¶æ€
        
        # è‡ªå®šä¹‰æ•°æ®é›†ä¸Šä¼ 
        else:
            st.subheader("ä¸Šä¼ æ•°æ®é›†")
            uploaded_file = st.file_uploader("ä¸Šä¼ CSVæˆ–Excelæ–‡ä»¶", type=["csv", "xls", "xlsx"])
            
            # æ•°æ®åŠ è½½
            if uploaded_file is not None:
                file_type = uploaded_file.name.split(".")[-1].lower()
                with st.spinner("æ­£åœ¨åŠ è½½æ•°æ®..."):
                    df, message = load_custom_dataset(uploaded_file, file_type)
                    if df is not None:
                        st.session_state.raw_data = df
                        st.success(f"âœ… {message}")
                        st.info(f"æ•°æ®é›†åŒ…å« {df.shape[0]} è¡Œ, {df.shape[1]} åˆ—")
                        
                        # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
                        with st.expander("æŸ¥çœ‹æ•°æ®é¢„è§ˆ"):
                            st.dataframe(df.head(10), use_container_width=True)
            
            # æ•°æ®æ¸…æ´—è®¾ç½®
            if st.session_state.raw_data is not None:
                st.subheader("3. æ•°æ®æ¸…æ´—è®¾ç½®")
                
                # æ˜¾ç¤ºç¼ºå¤±å€¼æƒ…å†µ
                missing_values = st.session_state.raw_data.isnull().sum()
                missing_values = missing_values[missing_values > 0]
                if not missing_values.empty:
                    st.warning("æ£€æµ‹åˆ°ç¼ºå¤±å€¼:")
                    st.write(missing_values)
                
                # ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥
                missing_strategy = st.selectbox(
                    "ç¼ºå¤±å€¼å¤„ç†æ–¹å¼",
                    ["åˆ é™¤è¡Œ", "å‡å€¼å¡«å……", "ä¸­ä½æ•°å¡«å……"]
                )
                
                # ç±»åˆ«ç‰¹å¾ç¼–ç æ–¹å¼
                st.subheader("4. ç‰¹å¾ç¼–ç è®¾ç½®")
                categorical_encoding = st.selectbox(
                    "ç±»åˆ«ç‰¹å¾ç¼–ç æ–¹å¼",
                    ["æ ‡ç­¾ç¼–ç ", "ç‹¬çƒ­ç¼–ç "]
                )
                
                # é€‰æ‹©ç›®æ ‡åˆ—ï¼ˆæ ‡ç­¾åˆ—ï¼‰
                st.subheader("5. é€‰æ‹©ç›®æ ‡åˆ—")
                target_col = st.selectbox(
                    "é€‰æ‹©ä½œä¸ºé¢„æµ‹ç›®æ ‡çš„åˆ—",
                    st.session_state.raw_data.columns.tolist()
                )
                
                # åˆ’åˆ†æ¯”ä¾‹
                test_size = st.slider(
                    "æµ‹è¯•é›†æ¯”ä¾‹",
                    min_value=0.1,
                    max_value=0.5,
                    value=0.3,
                    step=0.1
                )
                
                # å‡†å¤‡æ•°æ®æŒ‰é’®
                if st.button("å‡†å¤‡è®­ç»ƒæ•°æ®", use_container_width=True):
                    with st.spinner("æ­£åœ¨å¤„ç†æ•°æ®..."):
                        # æ¸…æ´—æ•°æ®
                        cleaned_df, all_target_names, label_encoder, numerical_cols, categorical_cols = clean_data(
                            st.session_state.raw_data, 
                            missing_strategy,
                            categorical_encoding,
                            target_col,
                            st.session_state.task_type
                        )
                        
                        # å‡†å¤‡è®­ç»ƒæ•°æ®
                        result = prepare_training_data(cleaned_df, target_col, test_size, st.session_state.task_type)
                        if result[0] is not None:
                            X_train, X_test, y_train, y_test, feature_names, unique_classes, msg = result
                            
                            # æ›´æ–°çŠ¶æ€
                            st.session_state.processed_data = cleaned_df
                            st.session_state.target_column = target_col
                            st.session_state.X_train = X_train
                            st.session_state.X_test = X_test
                            st.session_state.y_train = y_train
                            st.session_state.y_test = y_test
                            st.session_state.feature_names = feature_names
                            st.session_state.label_encoder = label_encoder  # ä¿å­˜æ ‡ç­¾ç¼–ç å™¨
                            st.session_state.numerical_cols = numerical_cols
                            st.session_state.categorical_cols = categorical_cols
                            
                            # è®¾ç½®ç›®æ ‡åç§°
                            if st.session_state.task_type == "classification":
                                if all_target_names is not None and label_encoder is not None:
                                    # æ˜ å°„å®é™…å­˜åœ¨çš„ç±»åˆ«åˆ°åŸå§‹åç§°
                                    st.session_state.target_names = [all_target_names[i] for i in unique_classes]
                                else:
                                    st.session_state.target_names = [f"ç±»åˆ« {i}" for i in unique_classes]
                            else:
                                st.session_state.target_names = ["ç›®æ ‡å€¼"]
                            
                            st.success(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {msg}")
                            st.write(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {X_train.shape[0]}")
                            st.write(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {X_test.shape[0]}")
                            st.write(f"ç‰¹å¾æ•°: {X_train.shape[1]}")
                            if st.session_state.task_type == "classification":
                                st.write(f"å®é™…ç±»åˆ«æ•°: {len(unique_classes)}")  # æ˜¾ç¤ºå®é™…ç±»åˆ«æ•°
                            st.session_state.is_trained = False  # é‡æ–°å‡†å¤‡æ•°æ®åé‡ç½®æ¨¡å‹çŠ¶æ€
        
        # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²åŠ è½½
        if (st.session_state.dataset is not None and st.session_state.dataset_type == "built_in") or \
           (st.session_state.processed_data is not None and st.session_state.dataset_type == "custom"):
            if st.session_state.dataset_type == "built_in":
                # ä½¿ç”¨æ›´å¯é çš„æ–¹å¼è¯†åˆ«æ•°æ®é›†åç§°
                name_key = st.session_state.dataset.get("name_key")
                dataset_name_map = {
                    "iris": "é¸¢å°¾èŠ±æ•°æ®é›†",
                    "breast_cancer": "ä¹³è…ºç™Œæ•°æ®é›†",
                    "wine": "è‘¡è„é…’æ•°æ®é›†",
                    "diabetes": "ç³–å°¿ç—…æ•°æ®é›†",
                    "california_housing": "åŠ å·æˆ¿ä»·æ•°æ®é›†"
                }
                dataset_name = dataset_name_map.get(name_key, "æœªçŸ¥æ•°æ®é›†")
                st.info(f"å·²åŠ è½½: {dataset_name}")
            else:
                st.info(f"å·²åŠ è½½: è‡ªå®šä¹‰æ•°æ®é›†")
            
            # åªæœ‰å½“X_trainå­˜åœ¨æ—¶æ‰æ˜¾ç¤ºè¿™äº›ä¿¡æ¯
            if st.session_state.X_train is not None:
                st.write(f"ç‰¹å¾æ•°: {st.session_state.X_train.shape[1]}")
                st.write(f"æ ·æœ¬æ•°: {st.session_state.X_train.shape[0] + st.session_state.X_test.shape[0]}")
                # æ˜¾ç¤ºå®é™…ç±»åˆ«æ•°ï¼ˆä»…åˆ†ç±»ä»»åŠ¡ï¼‰
                if st.session_state.task_type == "classification" and st.session_state.y_train is not None:
                    st.write(f"å®é™…ç±»åˆ«æ•°: {len(np.unique(st.session_state.y_train))}")
        
        st.markdown("---")
        
        # æ¨¡å‹é€‰æ‹©ï¼ˆä»…å½“æ•°æ®é›†å·²åŠ è½½æ—¶æ˜¾ç¤ºï¼‰
        if (st.session_state.dataset is not None and st.session_state.dataset_type == "built_in") or \
           (st.session_state.processed_data is not None and st.session_state.dataset_type == "custom"):
            st.subheader("6. é€‰æ‹©æ¨¡å‹")
            
            # æ ¹æ®ä»»åŠ¡ç±»å‹æ˜¾ç¤ºä¸åŒçš„æ¨¡å‹é€‰é¡¹
            if st.session_state.task_type == "classification":
                model_name = st.selectbox(
                    "é€‰æ‹©åˆ†ç±»æ¨¡å‹",
                    ["Kè¿‘é‚»åˆ†ç±»å™¨", "æ”¯æŒå‘é‡æœº", "å†³ç­–æ ‘", "éšæœºæ£®æ—", "é€»è¾‘å›å½’", "æ¢¯åº¦æå‡æ ‘", "XGBoost"]
                )
            else:
                model_name = st.selectbox(
                    "é€‰æ‹©å›å½’æ¨¡å‹",
                    ["Kè¿‘é‚»å›å½’å™¨", "æ”¯æŒå‘é‡å›å½’", "å†³ç­–æ ‘å›å½’", "éšæœºæ£®æ—å›å½’", "çº¿æ€§å›å½’", 
                     "å²­å›å½’", "Lassoå›å½’", "æ¢¯åº¦æå‡å›å½’", "XGBoostå›å½’"]
                )
            
            # æ¨¡å‹å‚æ•°é…ç½®
            st.subheader("7. æ¨¡å‹å‚æ•°")
            model_params = {}
            
            if model_name in ["Kè¿‘é‚»åˆ†ç±»å™¨", "Kè¿‘é‚»å›å½’å™¨"]:
                model_params["n_neighbors"] = st.slider("è¿‘é‚»æ•°é‡", 1, 20, 5)
            
            elif model_name in ["æ”¯æŒå‘é‡æœº", "æ”¯æŒå‘é‡å›å½’"]:
                model_params["C"] = st.slider("æ­£åˆ™åŒ–å‚æ•° C", 0.01, 10.0, 1.0)
                model_params["kernel"] = st.selectbox("æ ¸å‡½æ•°", ["linear", "poly", "rbf", "sigmoid"])
            
            elif model_name in ["å†³ç­–æ ‘", "å†³ç­–æ ‘å›å½’"]:
                model_params["max_depth"] = st.slider("æœ€å¤§æ·±åº¦", 1, 20, 5)
            
            elif model_name in ["éšæœºæ£®æ—", "éšæœºæ£®æ—å›å½’", "æ¢¯åº¦æå‡æ ‘", "æ¢¯åº¦æå‡å›å½’", "XGBoost", "XGBoostå›å½’"]:
                model_params["n_estimators"] = st.slider("æ ‘çš„æ•°é‡", 10, 200, 100)
            
            elif model_name == "é€»è¾‘å›å½’":
                model_params["C"] = st.slider("æ­£åˆ™åŒ–å‚æ•° C", 0.01, 10.0, 1.0)
            
            elif model_name in ["å²­å›å½’", "Lassoå›å½’"]:
                model_params["alpha"] = st.slider("æ­£åˆ™åŒ–ç³»æ•°", 0.01, 10.0, 1.0)
            
            # è¶…å‚æ•°è°ƒä¼˜é€‰é¡¹
            st.subheader("8. è¶…å‚æ•°è°ƒä¼˜")
            use_grid_search = st.checkbox("å¯ç”¨ç½‘æ ¼æœç´¢è°ƒä¼˜è¶…å‚æ•°", value=False)
            
            st.markdown("---")
            
            # è®­ç»ƒæ¨¡å‹æŒ‰é’®
            st.subheader("9. è®­ç»ƒæ¨¡å‹")
            if st.button("å¼€å§‹è®­ç»ƒ", use_container_width=True):
                with st.spinner("æ­£åœ¨å‡†å¤‡è®­ç»ƒ..."):
                    # åˆ›å»ºæ¨¡å‹
                    model = create_model(model_name, model_params, st.session_state.task_type)
                    if model is not None:
                        # å¦‚æœå¯ç”¨äº†ç½‘æ ¼æœç´¢ï¼Œåˆ™è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜
                        if use_grid_search:
                            # å®šä¹‰ç®€å•çš„å‚æ•°ç½‘æ ¼
                            param_grid = {}
                            if model_name in ["Kè¿‘é‚»åˆ†ç±»å™¨", "Kè¿‘é‚»å›å½’å™¨"]:
                                param_grid["n_neighbors"] = [3, 5, 7, 9]
                            elif model_name in ["éšæœºæ£®æ—", "éšæœºæ£®æ—å›å½’", "æ¢¯åº¦æå‡æ ‘", "æ¢¯åº¦æå‡å›å½’"]:
                                param_grid["n_estimators"] = [50, 100, 150]
                            
                            if param_grid:  # åªæœ‰å½“æœ‰å‚æ•°å¯ä¼˜åŒ–æ—¶æ‰è¿›è¡Œç½‘æ ¼æœç´¢
                                st.subheader("æ­£åœ¨è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜...")
                                model, best_params = tune_model(model, param_grid, 
                                                              st.session_state.X_train, 
                                                              st.session_state.y_train)
                                st.success(f"æœ€ä½³å‚æ•°: {best_params}")
                            else:
                                st.info("è¯¥æ¨¡å‹æš‚ä¸æ”¯æŒç½‘æ ¼æœç´¢è°ƒä¼˜ï¼Œå°†ä½¿ç”¨é»˜è®¤å‚æ•°è®­ç»ƒ")
                        
                        # è®­ç»ƒæ¨¡å‹
                        st.subheader("æ­£åœ¨è®­ç»ƒæ¨¡å‹...")
                        trained_model, training_time, error_msg = train_model(
                            model, 
                            st.session_state.X_train, 
                            st.session_state.y_train
                        )
                        
                        if error_msg:
                            st.error(error_msg)
                        elif trained_model is not None:
                            # è¯„ä¼°æ¨¡å‹
                            train_metrics, _ = evaluate_model(
                                trained_model, 
                                st.session_state.X_train, 
                                st.session_state.y_train,
                                st.session_state.task_type,
                                st.session_state.target_names
                            )
                            
                            test_metrics, y_pred = evaluate_model(
                                trained_model, 
                                st.session_state.X_test, 
                                st.session_state.y_test,
                                st.session_state.task_type,
                                st.session_state.target_names
                            )
                            
                            # æ›´æ–°ä¼šè¯çŠ¶æ€
                            st.session_state.model = trained_model
                            st.session_state.model_name = model_name
                            st.session_state.train_metrics = train_metrics
                            st.session_state.test_metrics = test_metrics
                            st.session_state.y_pred = y_pred
                            st.session_state.training_time = training_time
                            st.session_state.is_trained = True
                            
                            # ä¿å­˜åˆ°è®­ç»ƒå†å²
                            history_entry = {
                                "model_name": model_name,
                                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                                "train_metrics": train_metrics,
                                "test_metrics": test_metrics,
                                "training_time": training_time
                            }
                            st.session_state.training_history.append(history_entry)
                            
                            st.success("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
                            if st.session_state.task_type == "classification":
                                st.write(f"è®­ç»ƒå‡†ç¡®ç‡: {train_metrics.get('accuracy', 0):.4f}")
                                st.write(f"æµ‹è¯•å‡†ç¡®ç‡: {test_metrics.get('accuracy', 0):.4f}")
                            else:
                                st.write(f"è®­ç»ƒRÂ²åˆ†æ•°: {train_metrics.get('r2', 0):.4f}")
                                st.write(f"æµ‹è¯•RÂ²åˆ†æ•°: {test_metrics.get('r2', 0):.4f}")
                            st.write(f"è®­ç»ƒæ—¶é—´: {training_time:.4f}ç§’")
        
        # æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
        if st.session_state.is_trained:
            st.markdown("---")
            st.subheader("10. æ¨¡å‹ç®¡ç†")
            
            # ä¿å­˜æ¨¡å‹
            if st.button("ğŸ’¾ ä¿å­˜å½“å‰æ¨¡å‹", use_container_width=True):
                model_path = save_model(st.session_state.model, st.session_state.model_name)
                if model_path:
                    with open(model_path, "rb") as file:
                        st.download_button(
                            label="ä¸‹è½½æ¨¡å‹æ–‡ä»¶",
                            data=file,
                            file_name=f"{st.session_state.model_name}_{time.strftime('%Y%m%d_%H%M%S')}.pkl",
                            mime="application/octet-stream",
                            use_container_width=True
                        )
        
        # åŠ è½½å·²æœ‰æ¨¡å‹
        st.subheader("11. åŠ è½½å·²æœ‰æ¨¡å‹")
        uploaded_model = st.file_uploader("ä¸Šä¼ æ¨¡å‹æ–‡ä»¶ (.pkl)", type=["pkl"])
        if uploaded_model is not None:
            with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹..."):
                model = load_model(uploaded_model)
                if model is not None:
                    st.session_state.model = model
                    st.session_state.model_name = f"åŠ è½½çš„æ¨¡å‹: {uploaded_model.name}"
                    st.session_state.is_trained = True
                    
                    # è¯„ä¼°åŠ è½½çš„æ¨¡å‹
                    if st.session_state.X_test is not None and st.session_state.y_test is not None:
                        test_metrics, y_pred = evaluate_model(
                            model, 
                            st.session_state.X_test, 
                            st.session_state.y_test,
                            st.session_state.task_type,
                            st.session_state.target_names
                        )
                        st.session_state.test_metrics = test_metrics
                        st.session_state.y_pred = y_pred
                        st.success("âœ… æ¨¡å‹åŠ è½½æˆåŠŸå¹¶å®Œæˆè¯„ä¼°ï¼")
        
        st.markdown("---")
        
        # é‡ç½®ä¼šè¯æŒ‰é’®
        if st.button("ğŸ”„ é‡ç½®ä¼šè¯", use_container_width=True):
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            init_session_state()
            st.success("ä¼šè¯å·²é‡ç½®")
            st.rerun()
    
    # ä¸»å†…å®¹åŒºåŸŸ
    if (st.session_state.dataset is None and st.session_state.dataset_type == "built_in") or \
       (st.session_state.raw_data is None and st.session_state.dataset_type == "custom"):
        # åˆå§‹é¡µé¢æç¤º
        col1, col2 = st.columns(2)
        with col1:
            st.info("""
            ### æ¬¢è¿ä½¿ç”¨æœºå™¨å­¦ä¹ å¯è§†åŒ–å¹³å°
            
            è¿™ä¸ªå¹³å°å¯ä»¥å¸®åŠ©æ‚¨ï¼š
            - æ¢ç´¢å†…ç½®æ•°æ®é›†æˆ–ä¸Šä¼ è‡ªå·±çš„æ•°æ®
            - è¿›è¡ŒåŸºç¡€æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†
            - è®­ç»ƒå¤šç§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆåˆ†ç±»å’Œå›å½’ï¼‰
            - å¯è§†åŒ–æ¨¡å‹æ€§èƒ½å’Œç»“æœ
            - æ¯”è¾ƒä¸åŒæ¨¡å‹çš„è¡¨ç°
            - ä¿å­˜å’ŒåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
            
            è¯·ä»ä¾§è¾¹æ å¼€å§‹ï¼Œé€‰æ‹©ä»»åŠ¡ç±»å‹ã€æ•°æ®é›†ç±»å‹å¹¶åŠ è½½æ•°æ®ï¼Œç„¶åé€‰æ‹©æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚
            """)
        
        with col2:
            st.image("https://images.unsplash.com/photo-1517694712202-14dd9538aa97?ixlib=rb-4.0.3&auto=format&fit=crop&w=800&q=80", 
                     caption="æœºå™¨å­¦ä¹ ä¸æ•°æ®å¯è§†åŒ–")
    else:
        # æ•°æ®é›†æ¦‚è§ˆä¸ä¸‹è½½
        st.header("ğŸ“‹ æ•°æ®é›†æ¦‚è§ˆ")
        # æ·»åŠ ä¸‹è½½æŒ‰é’®è¡Œ
        col_download = st.columns(1)
        with col_download[0]:
            st.subheader("æ•°æ®é›†ä¸‹è½½")
            if st.session_state.dataset_type == "built_in":
                # å†…ç½®æ•°æ®é›†ä¸‹è½½
                name_key = st.session_state.dataset.get("name_key")
                dataset_name_map = {
                    "iris": "é¸¢å°¾èŠ±æ•°æ®é›†",
                    "breast_cancer": "ä¹³è…ºç™Œæ•°æ®é›†",
                    "wine": "è‘¡è„é…’æ•°æ®é›†",
                    "diabetes": "ç³–å°¿ç—…æ•°æ®é›†",
                    "california_housing": "åŠ å·æˆ¿ä»·æ•°æ®é›†"
                }
                dataset_name = dataset_name_map.get(name_key, "æœªçŸ¥æ•°æ®é›†")
                
                # åˆ›å»ºå†…ç½®æ•°æ®é›†çš„DataFrame
                if name_key == "california_housing":
                    df = pd.DataFrame(
                        data=st.session_state.dataset.data,
                        columns=st.session_state.dataset.feature_names
                    )
                    df['ç›®æ ‡å€¼'] = st.session_state.dataset.target
                else:
                    df = pd.DataFrame(
                        data=st.session_state.dataset.data,
                        columns=st.session_state.dataset.feature_names
                    )
                    if st.session_state.task_type == "classification":
                        df['ç±»åˆ«'] = [st.session_state.dataset.target_names[i] for i in st.session_state.dataset.target]
                    else:
                        df['ç›®æ ‡å€¼'] = st.session_state.dataset.target
                
                # æä¾›CSVå’ŒExcelä¸¤ç§ä¸‹è½½æ ¼å¼
                csv = df.to_csv(index=False)
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½CSVæ ¼å¼",
                    data=csv,
                    file_name=f"{dataset_name}.csv",
                    mime="text/csv",
                    use_container_width=True
                )
                
                # ç”ŸæˆExcelæ–‡ä»¶
                output = BytesIO()
                with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                    df.to_excel(writer, index=False, sheet_name=dataset_name)
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½Excelæ ¼å¼",
                    data=output.getvalue(),
                    file_name=f"{dataset_name}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True
                )
            else:
                # è‡ªå®šä¹‰æ•°æ®é›†ä¸‹è½½ï¼ˆæä¾›åŸå§‹æ•°æ®å’Œå¤„ç†åæ•°æ®ä¸¤ç§é€‰é¡¹ï¼‰
                download_option = st.radio(
                    "é€‰æ‹©ä¸‹è½½æ•°æ®ç±»å‹",
                    ["åŸå§‹æ•°æ®", "å¤„ç†åæ•°æ®"],
                    horizontal=True
                )
                
                if download_option == "åŸå§‹æ•°æ®" and st.session_state.raw_data is not None:
                    df = st.session_state.raw_data
                    filename_base = "è‡ªå®šä¹‰æ•°æ®é›†_åŸå§‹æ•°æ®"
                    
                    # æä¾›CSVå’ŒExcelä¸¤ç§ä¸‹è½½æ ¼å¼
                    csv = df.to_csv(index=False)
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½CSVæ ¼å¼",
                        data=csv,
                        file_name=f"{filename_base}.csv",
                        mime="text/csv",
                        use_container_width=True
                    )
                    
                    output = BytesIO()
                    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                        df.to_excel(writer, index=False, sheet_name="æ•°æ®é›†")
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½Excelæ ¼å¼",
                        data=output.getvalue(),
                        file_name=f"{filename_base}.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        use_container_width=True
                    )
                elif download_option == "å¤„ç†åæ•°æ®" and st.session_state.processed_data is not None:
                    df = st.session_state.processed_data
                    filename_base = "è‡ªå®šä¹‰æ•°æ®é›†_å¤„ç†åæ•°æ®"
                    
                    # æä¾›CSVå’ŒExcelä¸¤ç§ä¸‹è½½æ ¼å¼
                    csv = df.to_csv(index=False)
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½CSVæ ¼å¼",
                        data=csv,
                        file_name=f"{filename_base}.csv",
                        mime="text/csv",
                        use_container_width=True
                    )
                    
                    output = BytesIO()
                    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                        df.to_excel(writer, index=False, sheet_name="æ•°æ®é›†")
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½Excelæ ¼å¼",
                        data=output.getvalue(),
                        file_name=f"{filename_base}.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        use_container_width=True
                    )
                else:
                    # å¤„ç†åæ•°æ®å°šæœªå‡†å¤‡å¥½æ—¶æ˜¾ç¤ºæç¤º
                    if download_option == "å¤„ç†åæ•°æ®":
                        st.info("è¯·å…ˆå®Œæˆæ•°æ®æ¸…æ´—å¹¶ç‚¹å‡»'å‡†å¤‡è®­ç»ƒæ•°æ®'æŒ‰é’®ä»¥ç”Ÿæˆå¤„ç†åæ•°æ®")
        
        st.markdown("---")
        
        # æ•°æ®é›†è¯¦ç»†ä¿¡æ¯
        col1, col2 = st.columns(2)
        with col1:
            if st.session_state.dataset_type == "built_in":
                # ä½¿ç”¨name_keyè¯†åˆ«æ•°æ®é›†
                name_key = st.session_state.dataset.get("name_key")
                dataset_name_map = {
                    "iris": "é¸¢å°¾èŠ±æ•°æ®é›†",
                    "breast_cancer": "ä¹³è…ºç™Œæ•°æ®é›†",
                    "wine": "è‘¡è„é…’æ•°æ®é›†",
                    "diabetes": "ç³–å°¿ç—…æ•°æ®é›†",
                    "california_housing": "åŠ å·æˆ¿ä»·æ•°æ®é›†"
                }
                dataset_name = dataset_name_map.get(name_key, "æœªçŸ¥æ•°æ®é›†")
                
                st.subheader(f"{dataset_name} åŸºæœ¬ä¿¡æ¯")
                st.write(f"ç‰¹å¾æ•°é‡: {st.session_state.dataset.data.shape[1]}")
                st.write(f"æ ·æœ¬æ•°é‡: {st.session_state.dataset.data.shape[0]}")
                if st.session_state.task_type == "classification":
                    st.write(f"ç±»åˆ«æ•°é‡: {len(st.session_state.dataset.target_names)}")
                    st.write("ç±»åˆ«åç§°: " + ", ".join(st.session_state.dataset.target_names))
                
                # æ˜¾ç¤ºæ•°æ®é›†å‰å‡ è¡Œ
                if name_key == "california_housing":
                    df = pd.DataFrame(
                        data=st.session_state.dataset.data,
                        columns=st.session_state.dataset.feature_names
                    )
                    df['ç›®æ ‡å€¼'] = st.session_state.dataset.target
                else:
                    df = pd.DataFrame(
                        data=st.session_state.dataset.data,
                        columns=st.session_state.dataset.feature_names
                    )
                    if st.session_state.task_type == "classification":
                        df['ç±»åˆ«'] = [st.session_state.dataset.target_names[i] for i in st.session_state.dataset.target]
                    else:
                        df['ç›®æ ‡å€¼'] = st.session_state.dataset.target
                st.dataframe(df.head(10), use_container_width=True)
            else:
                st.subheader(f"è‡ªå®šä¹‰æ•°æ®é›† åŸºæœ¬ä¿¡æ¯")
                # æ˜¾ç¤ºåŸå§‹æ•°æ®ä¿¡æ¯
                st.write(f"åŸå§‹æ•°æ® - æ ·æœ¬æ•°é‡: {st.session_state.raw_data.shape[0]}")
                st.write(f"åŸå§‹æ•°æ® - åˆ—æ•°é‡: {st.session_state.raw_data.shape[1]}")
                
                # æ˜¾ç¤ºç‰¹å¾ç±»å‹ä¿¡æ¯
                if st.session_state.numerical_cols or st.session_state.categorical_cols:
                    st.write(f"æ•°å€¼å‹ç‰¹å¾æ•°é‡: {len(st.session_state.numerical_cols)}")
                    st.write(f"ç±»åˆ«å‹ç‰¹å¾æ•°é‡: {len(st.session_state.categorical_cols)}")
                
                # åªæœ‰å½“processed_dataå­˜åœ¨æ—¶æ‰æ˜¾ç¤ºå¤„ç†åçš„æ•°æ®ä¿¡æ¯
                if st.session_state.processed_data is not None:
                    st.write(f"å¤„ç†åæ•°æ® - ç‰¹å¾æ•°é‡: {st.session_state.processed_data.shape[1] - 1}")  # å‡1æ˜¯å› ä¸ºåŒ…å«ç›®æ ‡åˆ—
                    st.write(f"å¤„ç†åæ•°æ® - æ ·æœ¬æ•°é‡: {st.session_state.processed_data.shape[0]}")
                    # æ˜¾ç¤ºå®é™…ç±»åˆ«æ•°ï¼ˆä»…åˆ†ç±»ä»»åŠ¡ï¼‰
                    if st.session_state.task_type == "classification" and st.session_state.y_train is not None:
                        st.write(f"å¤„ç†åæ•°æ® - å®é™…ç±»åˆ«æ•°: {len(np.unique(st.session_state.y_train))}")
                    st.write(f"ç›®æ ‡åˆ—: {st.session_state.target_column if st.session_state.target_column is not None else 'æœªé€‰æ‹©'}")
                else:
                    st.info("è¯·å®Œæˆæ•°æ®æ¸…æ´—å¹¶ç‚¹å‡»'å‡†å¤‡è®­ç»ƒæ•°æ®'æŒ‰é’®ä»¥æŸ¥çœ‹å¤„ç†åçš„æ•°æ®ä¿¡æ¯")
                
                # æ˜¾ç¤ºæ•°æ®é›†å‰å‡ è¡Œï¼ˆä¼˜å…ˆæ˜¾ç¤ºå¤„ç†åçš„æ•°æ®ï¼‰
                if st.session_state.processed_data is not None:
                    st.write("å¤„ç†åçš„æ•°æ®é¢„è§ˆ:")
                    st.dataframe(st.session_state.processed_data.head(10), use_container_width=True)
                else:
                    st.write("åŸå§‹æ•°æ®é¢„è§ˆ:")
                    st.dataframe(st.session_state.raw_data.head(10), use_container_width=True)
        
        with col2:
            st.subheader("ç‰¹å¾ç›¸å…³æ€§åˆ†æ")
            # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
            if st.session_state.dataset_type == "built_in":
                corr_matrix = pd.DataFrame(st.session_state.dataset.data).corr()
                
                fig = px.imshow(
                    corr_matrix, 
                    labels=dict(color="ç›¸å…³æ€§"),
                    text_auto=True,
                    color_continuous_scale="rdbu",
                    zmin=-1, zmax=1
                )
                fig.update_layout(
                    title='ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾',
                    width=700,
                    height=600,
                )
                st.plotly_chart(fig, use_container_width=True)
            else:
                # æ£€æŸ¥æ˜¯å¦æœ‰å¤„ç†åçš„æ•°æ®ï¼Œå¦åˆ™ä½¿ç”¨åŸå§‹æ•°æ®
                if st.session_state.processed_data is not None:
                    # åªè®¡ç®—ç‰¹å¾åˆ—ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œæ’é™¤ç›®æ ‡åˆ—
                    feature_cols = [col for col in st.session_state.processed_data.columns if col != st.session_state.target_column]
                    corr_matrix = st.session_state.processed_data[feature_cols].corr()
                    
                    # ç»˜åˆ¶äº¤äº’å¼çƒ­åŠ›å›¾
                    fig = px.imshow(
                        corr_matrix, 
                        labels=dict(color="ç›¸å…³æ€§"),
                        text_auto=True,
                        color_continuous_scale="rdbu",
                        zmin=-1, zmax=1
                    )
                    fig.update_layout(
                        title='ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾ (å¤„ç†åæ•°æ®)',
                        width=700,
                        height=600,
                    )
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    # ä½¿ç”¨åŸå§‹æ•°æ®è®¡ç®—ç›¸å…³æ€§
                    numeric_cols = st.session_state.raw_data.select_dtypes(include=['number']).columns
                    if len(numeric_cols) >= 2:  # éœ€è¦è‡³å°‘ä¸¤åˆ—æ•°å€¼å‹æ•°æ®æ‰èƒ½è®¡ç®—ç›¸å…³æ€§
                        corr_matrix = st.session_state.raw_data[numeric_cols].corr()
                        
                        # ç»˜åˆ¶äº¤äº’å¼çƒ­åŠ›å›¾
                        fig = px.imshow(
                            corr_matrix, 
                            labels=dict(color="ç›¸å…³æ€§"),
                            text_auto=True,
                            color_continuous_scale="rdbu",
                            zmin=-1, zmax=1
                        )
                        fig.update_layout(
                            title='ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾ (åŸå§‹æ•°æ®)',
                            width=700,
                            height=600,
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.info("åŸå§‹æ•°æ®ä¸­æ•°å€¼å‹ç‰¹å¾ä¸è¶³ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§ã€‚è¯·å…ˆå®Œæˆæ•°æ®å¤„ç†æ­¥éª¤ã€‚")
        
        st.markdown("---")
        
        # ç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–
        st.header("ğŸ“ˆ ç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–")
        if st.session_state.feature_names is not None:
            feature_idx = st.selectbox(
                "é€‰æ‹©ç‰¹å¾æŸ¥çœ‹åˆ†å¸ƒ",
                range(len(st.session_state.feature_names)),
                format_func=lambda x: st.session_state.feature_names[x]
            )
            
            col1, col2 = st.columns(2)
            with col1:
                # ç‰¹å¾åˆ†å¸ƒç›´æ–¹å›¾
                if st.session_state.dataset_type == "built_in":
                    data = st.session_state.dataset.data[:, feature_idx]
                else:
                    data = st.session_state.processed_data[st.session_state.feature_names[feature_idx]].values
                
                fig = px.histogram(
                    x=data,
                    nbins=30,
                    labels={'x': st.session_state.feature_names[feature_idx]},
                    title=f'{st.session_state.feature_names[feature_idx]} åˆ†å¸ƒ',
                    marginal='box'  # æ·»åŠ ç®±çº¿å›¾ä½œä¸ºè¾¹é™…åˆ†å¸ƒ
                )
                fig.update_layout(
                    width=600,
                    height=400
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # ç‰¹å¾æŒ‰ç›®æ ‡å€¼åˆ†å¸ƒ
                if st.session_state.dataset_type == "built_in":
                    if st.session_state.task_type == "classification":
                        fig = px.box(
                            x=st.session_state.dataset.target,
                            y=st.session_state.dataset.data[:, feature_idx],
                            labels={'x': 'ç±»åˆ«', 'y': st.session_state.feature_names[feature_idx]},
                            title=f'{st.session_state.feature_names[feature_idx]} æŒ‰ç±»åˆ«åˆ†å¸ƒ'
                        )
                        fig.update_layout(
                            width=600,
                            height=400
                        )
                        # æ›´æ–°xè½´æ ‡ç­¾
                        if st.session_state.target_names is not None:
                            fig.update_xaxes(
                                ticktext=st.session_state.target_names,
                                tickvals=list(range(len(st.session_state.target_names)))
                            )
                    else:  # regression
                        fig = px.scatter(
                            x=st.session_state.dataset.target,
                            y=st.session_state.dataset.data[:, feature_idx],
                            labels={'x': 'ç›®æ ‡å€¼', 'y': st.session_state.feature_names[feature_idx]},
                            title=f'{st.session_state.feature_names[feature_idx]} ä¸ç›®æ ‡å€¼å…³ç³»',
                            trendline='ols'  # æ·»åŠ è¶‹åŠ¿çº¿
                        )
                        fig.update_layout(
                            width=600,
                            height=400
                        )
                else:
                    if st.session_state.task_type == "classification":
                        fig = px.box(
                            x=st.session_state.processed_data[st.session_state.target_column],
                            y=st.session_state.processed_data[st.session_state.feature_names[feature_idx]],
                            labels={'x': 'ç±»åˆ«', 'y': st.session_state.feature_names[feature_idx]},
                            title=f'{st.session_state.feature_names[feature_idx]} æŒ‰ç±»åˆ«åˆ†å¸ƒ'
                        )
                        fig.update_layout(
                            width=600,
                            height=400
                        )
                        # æ›´æ–°xè½´æ ‡ç­¾
                        if st.session_state.target_names is not None:
                            fig.update_xaxes(
                                ticktext=st.session_state.target_names,
                                tickvals=list(range(len(st.session_state.target_names)))
                            )
                    else:  # regression
                        fig = px.scatter(
                            x=st.session_state.processed_data[st.session_state.target_column],
                            y=st.session_state.processed_data[st.session_state.feature_names[feature_idx]],
                            labels={'x': 'ç›®æ ‡å€¼', 'y': st.session_state.feature_names[feature_idx]},
                            title=f'{st.session_state.feature_names[feature_idx]} ä¸ç›®æ ‡å€¼å…³ç³»',
                            trendline='ols'  # æ·»åŠ è¶‹åŠ¿çº¿
                        )
                        fig.update_layout(
                            width=600,
                            height=400
                        )
                
                st.plotly_chart(fig, use_container_width=True)
        else:
            # å½“ç‰¹å¾åç§°å°šæœªåŠ è½½æ—¶æ˜¾ç¤ºæç¤º
            if st.session_state.dataset_type == "custom" and st.session_state.raw_data is not None:
                st.info("è¯·å®Œæˆæ•°æ®æ¸…æ´—å¹¶ç‚¹å‡»'å‡†å¤‡è®­ç»ƒæ•°æ®'æŒ‰é’®ä»¥æŸ¥çœ‹ç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–")
        
        st.markdown("---")
        
        # æ¨¡å‹è®­ç»ƒç»“æœ
        if st.session_state.is_trained:
            st.header("ğŸ† æ¨¡å‹è®­ç»ƒç»“æœ")
            
            # æ¨¡å‹æ€§èƒ½æ¦‚è§ˆ
            st.subheader(f"{st.session_state.model_name} æ€§èƒ½æŒ‡æ ‡")
            
            if st.session_state.task_type == "classification":
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("è®­ç»ƒå‡†ç¡®ç‡", f"{st.session_state.train_metrics.get('accuracy', 0):.4f}")
                with col2:
                    st.metric("è®­ç»ƒç²¾ç¡®ç‡", f"{st.session_state.train_metrics.get('precision', 0):.4f}")
                with col3:
                    st.metric("è®­ç»ƒå¬å›ç‡", f"{st.session_state.train_metrics.get('recall', 0):.4f}")
                with col4:
                    st.metric("è®­ç»ƒF1åˆ†æ•°", f"{st.session_state.train_metrics.get('f1', 0):.4f}")
                
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("æµ‹è¯•å‡†ç¡®ç‡", f"{st.session_state.test_metrics.get('accuracy', 0):.4f}")
                with col2:
                    st.metric("æµ‹è¯•ç²¾ç¡®ç‡", f"{st.session_state.test_metrics.get('precision', 0):.4f}")
                with col3:
                    st.metric("æµ‹è¯•å¬å›ç‡", f"{st.session_state.test_metrics.get('recall', 0):.4f}")
                with col4:
                    st.metric("æµ‹è¯•F1åˆ†æ•°", f"{st.session_state.test_metrics.get('f1', 0):.4f}")
            else:  # regression
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("è®­ç»ƒMSE", f"{st.session_state.train_metrics.get('mse', 0):.4f}")
                with col2:
                    st.metric("è®­ç»ƒRMSE", f"{st.session_state.train_metrics.get('rmse', 0):.4f}")
                with col3:
                    st.metric("è®­ç»ƒMAE", f"{st.session_state.train_metrics.get('mae', 0):.4f}")
                with col4:
                    st.metric("è®­ç»ƒRÂ²åˆ†æ•°", f"{st.session_state.train_metrics.get('r2', 0):.4f}")
                
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("æµ‹è¯•MSE", f"{st.session_state.test_metrics.get('mse', 0):.4f}")
                with col2:
                    st.metric("æµ‹è¯•RMSE", f"{st.session_state.test_metrics.get('rmse', 0):.4f}")
                with col3:
                    st.metric("æµ‹è¯•MAE", f"{st.session_state.test_metrics.get('mae', 0):.4f}")
                with col4:
                    st.metric("æµ‹è¯•RÂ²åˆ†æ•°", f"{st.session_state.test_metrics.get('r2', 0):.4f}")
            
            st.write(f"**è®­ç»ƒæ—¶é—´:** {st.session_state.training_time:.4f} ç§’")
            
            st.markdown("---")
            
            # è¯¦ç»†è¯„ä¼°æŒ‡æ ‡
            st.subheader("è¯¦ç»†è¯„ä¼°æŠ¥å‘Š")
            if st.session_state.task_type == "classification":
                # è·å–å®é™…å­˜åœ¨çš„ç±»åˆ«
                actual_classes = np.unique(st.session_state.y_test)
                report = classification_report(
                    st.session_state.y_test,
                    st.session_state.y_pred,
                    labels=actual_classes,  # æ˜ç¡®æŒ‡å®šå®é™…å­˜åœ¨çš„ç±»åˆ«
                    target_names=st.session_state.target_names,
                    output_dict=True
                )
                report_df = pd.DataFrame(report).transpose()
                st.dataframe(report_df, use_container_width=True)
            else:
                # å›å½’ä»»åŠ¡çš„è¯„ä¼°æŠ¥å‘Š
                metrics_df = pd.DataFrame({
                    'æŒ‡æ ‡': ['MSE', 'RMSE', 'MAE', 'RÂ²åˆ†æ•°'],
                    'è®­ç»ƒé›†': [
                        f"{st.session_state.train_metrics.get('mse', 0):.4f}",
                        f"{st.session_state.train_metrics.get('rmse', 0):.4f}",
                        f"{st.session_state.train_metrics.get('mae', 0):.4f}",
                        f"{st.session_state.train_metrics.get('r2', 0):.4f}"
                    ],
                    'æµ‹è¯•é›†': [
                        f"{st.session_state.test_metrics.get('mse', 0):.4f}",
                        f"{st.session_state.test_metrics.get('rmse', 0):.4f}",
                        f"{st.session_state.test_metrics.get('mae', 0):.4f}",
                        f"{st.session_state.test_metrics.get('r2', 0):.4f}"
                    ]
                })
                st.dataframe(metrics_df, use_container_width=True)
            
            st.markdown("---")
            
            # å¯è§†åŒ–ç»“æœ
            st.subheader("ç»“æœå¯è§†åŒ–")
            
            # æ··æ·†çŸ©é˜µï¼ˆä»…åˆ†ç±»ä»»åŠ¡ï¼‰
            if st.session_state.task_type == "classification":
                st.subheader("æ··æ·†çŸ©é˜µ")
                cm_fig = plot_confusion_matrix(
                    st.session_state.y_test,
                    st.session_state.y_pred,
                    st.session_state.target_names
                )
                st.plotly_chart(cm_fig, use_container_width=True)
            
            # å­¦ä¹ æ›²çº¿
            st.subheader("å­¦ä¹ æ›²çº¿")
            lc_fig = plot_learning_curve(
                st.session_state.model,
                st.session_state.X_train,
                st.session_state.y_train,
                f"{st.session_state.model_name} å­¦ä¹ æ›²çº¿"
            )
            st.plotly_chart(lc_fig, use_container_width=True)
            
            # ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
            if hasattr(st.session_state.model, 'feature_importances_'):
                st.subheader("ç‰¹å¾é‡è¦æ€§")
                fi_fig = plot_feature_importance(
                    st.session_state.model,
                    st.session_state.feature_names
                )
                if fi_fig:
                    st.plotly_chart(fi_fig, use_container_width=True)
            
            # ROCæ›²çº¿ï¼ˆä»…äºŒåˆ†ç±»é—®é¢˜ï¼‰
            if st.session_state.task_type == "classification" and len(np.unique(st.session_state.y_test)) == 2:
                st.subheader("ROCæ›²çº¿")
                roc_fig = plot_roc_curve(
                    st.session_state.model,
                    st.session_state.X_test,
                    st.session_state.y_test,
                    st.session_state.target_names
                )
                if roc_fig:
                    st.plotly_chart(roc_fig, use_container_width=True)
            
            # å›å½’ä»»åŠ¡çš„å¯è§†åŒ–
            if st.session_state.task_type == "regression":
                st.subheader("é¢„æµ‹å€¼ vs çœŸå®å€¼")
                pred_fig = plot_prediction_vs_actual(
                    st.session_state.y_test,
                    st.session_state.y_pred
                )
                st.plotly_chart(pred_fig, use_container_width=True)
                
                st.subheader("æ®‹å·®å›¾")
                residual_fig = plot_residuals(
                    st.session_state.y_test,
                    st.session_state.y_pred
                )
                st.plotly_chart(residual_fig, use_container_width=True)
            
            st.markdown("---")
            
            # é¢„æµ‹ç¤ºä¾‹
            st.subheader("é¢„æµ‹ç¤ºä¾‹")
            if len(st.session_state.X_test) > 0:
                sample_idx = st.slider(
                    "é€‰æ‹©æµ‹è¯•é›†ä¸­çš„æ ·æœ¬æŸ¥çœ‹é¢„æµ‹ç»“æœ",
                    0, len(st.session_state.X_test) - 1, 0
                )
                
                sample = st.session_state.X_test[sample_idx:sample_idx+1]
                true_label = st.session_state.y_test[sample_idx]
                pred_label = st.session_state.y_pred[sample_idx]
                
                col1, col2 = st.columns(2)
                with col1:
                    st.write("**æ ·æœ¬ç‰¹å¾å€¼:**")
                    features_df = pd.DataFrame(
                        sample, 
                        columns=st.session_state.feature_names
                    )
                    st.dataframe(features_df, use_container_width=True)
                
                with col2:
                    st.write("**é¢„æµ‹ç»“æœ:**")
                    if st.session_state.task_type == "classification":
                        actual_classes = np.unique(st.session_state.y_test)
                        if true_label == pred_label:
                            st.success(f"æ­£ç¡®é¢„æµ‹: {st.session_state.target_names[np.where(actual_classes == true_label)[0][0]]}")
                        else:
                            st.error(f"é¢„æµ‹é”™è¯¯: é¢„æµ‹ä¸º {st.session_state.target_names[np.where(actual_classes == pred_label)[0][0]]}, å®é™…ä¸º {st.session_state.target_names[np.where(actual_classes == true_label)[0][0]]}")
                        
                        # æ˜¾ç¤ºé¢„æµ‹æ¦‚ç‡ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
                        if hasattr(st.session_state.model, 'predict_proba'):
                            proba = st.session_state.model.predict_proba(sample)[0]
                            proba_df = pd.DataFrame({
                                'ç±»åˆ«': st.session_state.target_names,
                                'æ¦‚ç‡': [f"{p:.4f}" for p in proba]
                            })
                            st.dataframe(proba_df, use_container_width=True)
                    else:  # regression
                        st.write(f"çœŸå®å€¼: {true_label:.4f}")
                        st.write(f"é¢„æµ‹å€¼: {pred_label:.4f}")
                        st.write(f"è¯¯å·®: {abs(true_label - pred_label):.4f}")
        
        else:
            # æç¤ºè®­ç»ƒæ¨¡å‹
            st.info("""
            ### ç­‰å¾…æ¨¡å‹è®­ç»ƒ
            
            è¯·åœ¨ä¾§è¾¹æ é€‰æ‹©ä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹å¹¶ç‚¹å‡»"å¼€å§‹è®­ç»ƒ"æŒ‰é’®ã€‚
            
            è®­ç»ƒå®Œæˆåï¼Œè¿™é‡Œå°†æ˜¾ç¤ºæ¨¡å‹æ€§èƒ½æŒ‡æ ‡å’Œå¯è§†åŒ–ç»“æœã€‚
            """)
            st.image("https://images.unsplash.com/photo-1517694712202-14dd9538aa97?ixlib=rb-4.0.3&auto=format&fit=crop&w=800&q=80", 
                     caption="æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ")
        
        # è®­ç»ƒå†å²è®°å½•
        if st.session_state.training_history:
            st.markdown("---")
            st.header("ğŸ“œ è®­ç»ƒå†å²è®°å½•")
            with st.expander(f"æŸ¥çœ‹ {len(st.session_state.training_history)} æ¡è®­ç»ƒè®°å½•"):
                for i, entry in enumerate(reversed(st.session_state.training_history)):
                    st.subheader(f"è®­ç»ƒè®°å½• {len(st.session_state.training_history) - i} ({entry['timestamp']})")
                    st.write(f"æ¨¡å‹: {entry['model_name']}")
                    st.write(f"è®­ç»ƒæ—¶é—´: {entry['training_time']:.4f}ç§’")
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        st.write("**è®­ç»ƒé›†æŒ‡æ ‡:**")
                        metrics_df = pd.DataFrame(list(entry['train_metrics'].items()), columns=['æŒ‡æ ‡', 'å€¼'])
                        metrics_df['å€¼'] = metrics_df['å€¼'].apply(lambda x: f"{x:.4f}")
                        st.dataframe(metrics_df, use_container_width=True)
                    
                    with col2:
                        st.write("**æµ‹è¯•é›†æŒ‡æ ‡:**")
                        metrics_df = pd.DataFrame(list(entry['test_metrics'].items()), columns=['æŒ‡æ ‡', 'å€¼'])
                        metrics_df['å€¼'] = metrics_df['å€¼'].apply(lambda x: f"{x:.4f}")
                        st.dataframe(metrics_df, use_container_width=True)
                    
                    st.markdown("---")
    
    # é¡µè„šä¿¡æ¯
    st.markdown("---")
    st.caption("2025.8.20ç‰ˆæœ¬ | å‡çº§äº†å†…ç½®æ•°æ®é›†æœºå™¨å­¦ä¹ åŠå…¶å¯è§†åŒ–åŠŸèƒ½, è‡ªå®šä¹‰æ•°æ®é›†åŠŸèƒ½ä»åœ¨è°ƒè¯•ä¸­")

if __name__ == "__main__":
    main()
